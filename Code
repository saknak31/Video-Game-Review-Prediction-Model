import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import SelectFromModel
from sklearn.impute import SimpleImputer
from xgboost import XGBRegressor

# Load and preprocess data
video_games_df = pd.read_csv('video_games.csv', encoding='utf-8')
video_games_df['release_date'] = pd.to_datetime(video_games_df['release_date'], errors='coerce')

# Clean user_review column
video_games_df['user_review'] = pd.to_numeric(video_games_df['user_review'], errors='coerce')

# Drop rows with NaN in essential columns
video_games_df = video_games_df.dropna(subset=['release_date', 'user_review'])

# Feature Engineering
video_games_df['release_year'] = video_games_df['release_date'].dt.year
video_games_df['release_month'] = video_games_df['release_date'].dt.month
video_games_df['game_age'] = pd.Timestamp.now().year - video_games_df['release_year']
video_games_df['is_holiday_release'] = video_games_df['release_month'].isin([11, 12]).astype(int)

# Prepare data for the model
features = ['platform', 'release_year', 'release_month', 'game_age', 'is_holiday_release']
X = video_games_df[features]
y = video_games_df['user_review']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing
categorical_features = ['platform']
numeric_features = [f for f in X.columns if f not in categorical_features]

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ])

# Create the pipeline
xgb = XGBRegressor(random_state=42)
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('feature_selection', SelectFromModel(xgb, threshold='median')),
    ('regressor', xgb)
])

# Hyperparameter tuning
param_dist = {
    'regressor__n_estimators': [100, 200, 300],
    'regressor__max_depth': [3, 4, 5, 6],
    'regressor__learning_rate': [0.01, 0.1, 0.2],
    'regressor__subsample': [0.8, 0.9, 1.0],
    'regressor__colsample_bytree': [0.8, 0.9, 1.0]
}

random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, 
                                   n_iter=20, cv=5, random_state=42, n_jobs=-1, 
                                   scoring='neg_mean_squared_error', verbose=2)

random_search.fit(X_train, y_train)

# Evaluate the model
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Best parameters: {random_search.best_params_}')
print(f'Test Mean Squared Error: {mse}')
print(f'Test R^2 Score: {r2}')

# Feature importance
feature_importance = best_model.named_steps['regressor'].feature_importances_
feature_names = (best_model.named_steps['preprocessor']
                 .named_transformers_['cat']
                 .named_steps['onehot']
                 .get_feature_names_out(categorical_features).tolist() + numeric_features)
feature_importance_dict = dict(zip(feature_names, feature_importance))
sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)
print("\nFeature Importance:")
for feature, importance in sorted_features[:10]:  # Print top 10 features
    print(f"{feature}: {importance}")
